{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataset\n",
    "# import keys_union\n",
    "# import keys_keras\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os, sys\n",
    "sys.path.insert(0, os.getcwd())\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 32, None, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, None, 64) 640         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 16, None, 64) 0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 16, None, 128 73856       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 8, None, 128) 0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 8, None, 256) 295168      pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 8, None, 256) 590080      conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 8, None, 256) 0           conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 4, None, 256) 0           zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 4, None, 512) 1180160     pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 4, None, 512) 16          conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 4, None, 512) 2359808     batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, None, 512) 16          conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 4, None, 512) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 2, None, 512) 0           zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv7 (Conv2D)                  (None, 1, None, 512) 1049088     pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, None, 1, 512) 0           conv7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "timedistrib (TimeDistributed)   (None, None, 512)    0           permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "blstm1 (Bidirectional)          (None, None, 512)    1181184     timedistrib[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "blstm1_out (Dense)              (None, None, 256)    131328      blstm1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "blstm2 (Bidirectional)          (None, None, 512)    787968      blstm1_out[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "blstm2_out_union (Dense)        (None, None, 69)     35397       blstm2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           blstm2_out_union[0][0]           \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,684,709\n",
      "Trainable params: 7,684,693\n",
      "Non-trainable params: 16\n",
      "__________________________________________________________________________________________________\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "characters = ' 0123456789-,./%￥|'\n",
    "# characters = keys_keras.alphabet[:]\n",
    "from model import get_model\n",
    "nclass = len(characters) + 1\n",
    "trainroot = '../data/lmdb/train_num_no_char_v2/'\n",
    "valroot = '../data/lmdb/valid_num_no_char_v2/'\n",
    "# modelPath = '../pretrain-models/keras.hdf5'\n",
    "modelPath = ''\n",
    "modelPath = '/mnt/wuwenhui/git_ocr_project/keras_crnn/save_model/num_char_000077_10.h5'\n",
    "workers = 4\n",
    "imgH = 32\n",
    "imgW = 256\n",
    "keep_ratio = False\n",
    "random_sample = False\n",
    "batchSize = 32\n",
    "\n",
    "testSize = 128\n",
    "n_len = 10\n",
    "loss = 100000\n",
    "interval = 200\n",
    "LEARNING_RATE = 0.001\n",
    "Learning_decay_step = 5000\n",
    "PERCEPTION = 0.25\n",
    "EPOCH_NUMS = 100000\n",
    "MODEL_PATH = '/mnt/wuwenhui/git_ocr_project/keras_crnn/save_model'\n",
    "\n",
    "LOG_FILE = 'log.txt'\n",
    "SUMMARY_PATH = './log/'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print('Creating save model path!!')\n",
    "    os.makedirs(MODEL_PATH)\n",
    "if not os.path.exists(SUMMARY_PATH):\n",
    "    os.makedirs(SUMMARY_PATH)\n",
    "\n",
    "model, basemodel = get_model(\n",
    "    height=imgH, nclass=nclass, learning_rate=LEARNING_RATE)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = PERCEPTION\n",
    "KTF.set_session(tf.Session(config=config))\n",
    "\n",
    "# 加载预训练参数\n",
    "if os.path.exists(modelPath):\n",
    "    # basemodel.load_weights(modelPath)\n",
    "    model.load_weights(modelPath,by_name=True)\n",
    "    print('model loaded')\n",
    "\n",
    "# plot_model(basemodel, to_file='basemodel.png')\n",
    "# plot_model(model, to_file='model.png')\n",
    "\n",
    "\n",
    "def one_hot(text, length=10, characters=characters):\n",
    "    label = np.zeros(length)\n",
    "    for i, char in enumerate(text):\n",
    "        index = characters.find(char)\n",
    "        if index == -1:\n",
    "            index = characters.find(u' ')\n",
    "        if i < length:\n",
    "            label[i] = index\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nSamples:13254\n",
      "nSamples:1325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 导入数据\n",
    "if random_sample:\n",
    "    sampler = dataset.randomSequentialSampler(train_dataset, batchSize)\n",
    "else:\n",
    "    sampler = None\n",
    "train_dataset = dataset.lmdbDataset(root=trainroot, target_transform=one_hot)\n",
    "# print(len(train_dataset))\n",
    "\n",
    "test_dataset = dataset.lmdbDataset(\n",
    "    root=valroot,\n",
    "    transform=dataset.resizeNormalize((imgW, imgH)),\n",
    "    target_transform=one_hot)\n",
    "\n",
    "# 生成训练用数据\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batchSize,\n",
    "    shuffle=True,\n",
    "#     sampler=sampler,\n",
    "    num_workers=2,\n",
    "    collate_fn=dataset.alignCollate(\n",
    "        imgH=imgH, imgW=imgW, keep_ratio=keep_ratio))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=testSize, \n",
    "    num_workers=2,\n",
    "    shuffle=False, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_data = iter(train_loader)\n",
    "# %time x,y = next(iter_data)\n",
    "# # print(y)\n",
    "# x = x.numpy()\n",
    "# X = x.reshape((-1, imgH, imgW, 1))\n",
    "# Y = np.array(y)\n",
    "# Length = int(imgW / 4) - 2\n",
    "# batch = X.shape[0]\n",
    "# X_train, Y_train = [\n",
    "#     X, Y, np.ones(batch) * Length,\n",
    "#     np.ones(batch) * n_len], np.ones(batch)\n",
    "# X_train[0].shape,X_train[1].shape,X_train[2].shape,X_train[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit x,y = next(iter_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!!\n",
      "32/32 [==============================] - 1s 38ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-06:52:10]--Step/Epoch/Total: [200/0/100000]\n",
      "\tTraining Loss is: [0.02093873918056488]\n",
      "\tVal Loss is: [8.46971432864666]\n",
      "\tSpeed is: [524.312577152054] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "\tSave model to disk: /mnt/wuwenhui/git_ocr_project/keras_crnn/save_model/num_char_000084_10.h5\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-06:53:36]--Step/Epoch/Total: [400/0/100000]\n",
      "\tTraining Loss is: [0.04130443185567856]\n",
      "\tVal Loss is: [7.997243277728558]\n",
      "\tSpeed is: [517.2185916371791] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "\tSave model to disk: /mnt/wuwenhui/git_ocr_project/keras_crnn/save_model/num_char_000079_10.h5\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-06:55:00]--Step/Epoch/Total: [600/1/100000]\n",
      "\tTraining Loss is: [0.014036129228770733]\n",
      "\tVal Loss is: [8.19473285973072]\n",
      "\tSpeed is: [515.7437379611646] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-06:56:23]--Step/Epoch/Total: [800/1/100000]\n",
      "\tTraining Loss is: [0.008582988753914833]\n",
      "\tVal Loss is: [7.950836397707462]\n",
      "\tSpeed is: [525.3758563826223] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "\tSave model to disk: /mnt/wuwenhui/git_ocr_project/keras_crnn/save_model/num_char_000079_10.h5\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-06:57:46]--Step/Epoch/Total: [1000/2/100000]\n",
      "\tTraining Loss is: [0.0065952786244452]\n",
      "\tVal Loss is: [7.979076657444239]\n",
      "\tSpeed is: [465.9022897982125] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-06:59:08]--Step/Epoch/Total: [1200/2/100000]\n",
      "\tTraining Loss is: [0.0025460070464760065]\n",
      "\tVal Loss is: [8.123007077723742]\n",
      "\tSpeed is: [495.19761604541225] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:00:32]--Step/Epoch/Total: [1400/3/100000]\n",
      "\tTraining Loss is: [0.00318882311694324]\n",
      "\tVal Loss is: [8.07320912182331]\n",
      "\tSpeed is: [519.634027739062] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:01:54]--Step/Epoch/Total: [1600/3/100000]\n",
      "\tTraining Loss is: [0.016086271032691002]\n",
      "\tVal Loss is: [8.032801803201437]\n",
      "\tSpeed is: [526.0611548801679] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:03:17]--Step/Epoch/Total: [1800/4/100000]\n",
      "\tTraining Loss is: [0.004276732914149761]\n",
      "\tVal Loss is: [8.075000505894423]\n",
      "\tSpeed is: [512.7222674519526] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:04:40]--Step/Epoch/Total: [2000/4/100000]\n",
      "\tTraining Loss is: [0.0037539268378168344]\n",
      "\tVal Loss is: [8.06132485345006]\n",
      "\tSpeed is: [523.8420794298853] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:06:03]--Step/Epoch/Total: [2200/5/100000]\n",
      "\tTraining Loss is: [0.0020826109685003757]\n",
      "\tVal Loss is: [8.012893073260784]\n",
      "\tSpeed is: [489.87919174623903] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:07:26]--Step/Epoch/Total: [2400/5/100000]\n",
      "\tTraining Loss is: [0.0012664822861552238]\n",
      "\tVal Loss is: [8.065671999007463]\n",
      "\tSpeed is: [507.22341910802146] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:08:50]--Step/Epoch/Total: [2600/6/100000]\n",
      "\tTraining Loss is: [0.007432596292346716]\n",
      "\tVal Loss is: [8.07951883226633]\n",
      "\tSpeed is: [502.9325412979731] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:10:13]--Step/Epoch/Total: [2800/6/100000]\n",
      "\tTraining Loss is: [0.002253846963867545]\n",
      "\tVal Loss is: [8.100274469703436]\n",
      "\tSpeed is: [503.69384743049494] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:11:36]--Step/Epoch/Total: [3000/7/100000]\n",
      "\tTraining Loss is: [0.028037970885634422]\n",
      "\tVal Loss is: [8.067790113389492]\n",
      "\tSpeed is: [507.209618145034] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:12:58]--Step/Epoch/Total: [3200/7/100000]\n",
      "\tTraining Loss is: [0.00841706246137619]\n",
      "\tVal Loss is: [8.11229170113802]\n",
      "\tSpeed is: [508.8723482987255] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:14:22]--Step/Epoch/Total: [3400/8/100000]\n",
      "\tTraining Loss is: [0.0050039142370224]\n",
      "\tVal Loss is: [8.112194873392582]\n",
      "\tSpeed is: [502.5098511868748] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:15:44]--Step/Epoch/Total: [3600/8/100000]\n",
      "\tTraining Loss is: [0.003960457164794207]\n",
      "\tVal Loss is: [8.088327340781689]\n",
      "\tSpeed is: [445.8131780646903] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:17:07]--Step/Epoch/Total: [3800/9/100000]\n",
      "\tTraining Loss is: [0.0011609471403062344]\n",
      "\tVal Loss is: [8.084241822361946]\n",
      "\tSpeed is: [526.1370428700363] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:18:30]--Step/Epoch/Total: [4000/9/100000]\n",
      "\tTraining Loss is: [0.005934215150773525]\n",
      "\tVal Loss is: [8.118012674152851]\n",
      "\tSpeed is: [504.48173190435267] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:19:54]--Step/Epoch/Total: [4200/10/100000]\n",
      "\tTraining Loss is: [0.0017096438677981496]\n",
      "\tVal Loss is: [8.140537593513727]\n",
      "\tSpeed is: [504.41711815180724] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:21:17]--Step/Epoch/Total: [4400/10/100000]\n",
      "\tTraining Loss is: [0.0027444246225059032]\n",
      "\tVal Loss is: [8.124028071761131]\n",
      "\tSpeed is: [497.2538614701832] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:22:40]--Step/Epoch/Total: [4600/11/100000]\n",
      "\tTraining Loss is: [0.013786613009870052]\n",
      "\tVal Loss is: [8.147108115255833]\n",
      "\tSpeed is: [497.8086258023971] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:24:03]--Step/Epoch/Total: [4800/11/100000]\n",
      "\tTraining Loss is: [0.0009285948472097516]\n",
      "\tVal Loss is: [8.140407644212246]\n",
      "\tSpeed is: [504.2488360979451] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:25:26]--Step/Epoch/Total: [5000/12/100000]\n",
      "\tTraining Loss is: [0.01901000738143921]\n",
      "\tVal Loss is: [8.110888324677944]\n",
      "\tSpeed is: [498.506490573752] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:26:49]--Step/Epoch/Total: [5200/12/100000]\n",
      "\tTraining Loss is: [0.0019014475401490927]\n",
      "\tVal Loss is: [8.163020078092813]\n",
      "\tSpeed is: [495.29673433696274] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:28:12]--Step/Epoch/Total: [5400/13/100000]\n",
      "\tTraining Loss is: [0.003746691858395934]\n",
      "\tVal Loss is: [8.131613235920668]\n",
      "\tSpeed is: [509.0045035260024] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:29:36]--Step/Epoch/Total: [5600/13/100000]\n",
      "\tTraining Loss is: [0.008648285642266273]\n",
      "\tVal Loss is: [8.123429343104362]\n",
      "\tSpeed is: [511.0181365394128] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:30:59]--Step/Epoch/Total: [5800/13/100000]\n",
      "\tTraining Loss is: [0.004438498057425022]\n",
      "\tVal Loss is: [8.148283991962671]\n",
      "\tSpeed is: [430.33818189653135] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:32:22]--Step/Epoch/Total: [6000/14/100000]\n",
      "\tTraining Loss is: [0.0025873971171677113]\n",
      "\tVal Loss is: [8.13744181022048]\n",
      "\tSpeed is: [401.6682292349991] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:33:45]--Step/Epoch/Total: [6200/14/100000]\n",
      "\tTraining Loss is: [0.001080782851204276]\n",
      "\tVal Loss is: [8.163531385362148]\n",
      "\tSpeed is: [478.3625084040969] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:35:08]--Step/Epoch/Total: [6400/15/100000]\n",
      "\tTraining Loss is: [0.0038581506814807653]\n",
      "\tVal Loss is: [8.140265613794327]\n",
      "\tSpeed is: [544.5447233688699] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:36:32]--Step/Epoch/Total: [6600/15/100000]\n",
      "\tTraining Loss is: [0.0020812898874282837]\n",
      "\tVal Loss is: [8.12955268099904]\n",
      "\tSpeed is: [499.077713166534] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:37:55]--Step/Epoch/Total: [6800/16/100000]\n",
      "\tTraining Loss is: [0.0013146827695891261]\n",
      "\tVal Loss is: [8.16052520275116]\n",
      "\tSpeed is: [532.4633085232387] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:39:19]--Step/Epoch/Total: [7000/16/100000]\n",
      "\tTraining Loss is: [0.0023814490996301174]\n",
      "\tVal Loss is: [8.163331542164087]\n",
      "\tSpeed is: [559.7932358548674] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:40:42]--Step/Epoch/Total: [7200/17/100000]\n",
      "\tTraining Loss is: [0.006383567117154598]\n",
      "\tVal Loss is: [8.149050867184997]\n",
      "\tSpeed is: [523.8780654138137] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:42:05]--Step/Epoch/Total: [7400/17/100000]\n",
      "\tTraining Loss is: [0.0011747488752007484]\n",
      "\tVal Loss is: [8.156067982316017]\n",
      "\tSpeed is: [451.92468898758534] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:43:28]--Step/Epoch/Total: [7600/18/100000]\n",
      "\tTraining Loss is: [0.0029153688810765743]\n",
      "\tVal Loss is: [8.173571519553661]\n",
      "\tSpeed is: [534.6368996366542] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:44:51]--Step/Epoch/Total: [7800/18/100000]\n",
      "\tTraining Loss is: [0.0012338971719145775]\n",
      "\tVal Loss is: [8.178628269582987]\n",
      "\tSpeed is: [496.24107475612715] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:46:14]--Step/Epoch/Total: [8000/19/100000]\n",
      "\tTraining Loss is: [0.001194332493469119]\n",
      "\tVal Loss is: [8.169236727058887]\n",
      "\tSpeed is: [474.48917487403304] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:47:36]--Step/Epoch/Total: [8200/19/100000]\n",
      "\tTraining Loss is: [0.0017266720533370972]\n",
      "\tVal Loss is: [8.149401156231761]\n",
      "\tSpeed is: [482.70034813131167] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:49:00]--Step/Epoch/Total: [8400/20/100000]\n",
      "\tTraining Loss is: [0.003363983239978552]\n",
      "\tVal Loss is: [8.173522289842367]\n",
      "\tSpeed is: [494.37025695607156] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:50:23]--Step/Epoch/Total: [8600/20/100000]\n",
      "\tTraining Loss is: [0.0010514804162085056]\n",
      "\tVal Loss is: [8.212019510567188]\n",
      "\tSpeed is: [513.0148199655201] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:51:46]--Step/Epoch/Total: [8800/21/100000]\n",
      "\tTraining Loss is: [0.0016873794374987483]\n",
      "\tVal Loss is: [8.215710144490004]\n",
      "\tSpeed is: [486.46447495162096] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:53:09]--Step/Epoch/Total: [9000/21/100000]\n",
      "\tTraining Loss is: [0.001783457351848483]\n",
      "\tVal Loss is: [8.17780078575015]\n",
      "\tSpeed is: [501.7053035880573] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:54:31]--Step/Epoch/Total: [9200/22/100000]\n",
      "\tTraining Loss is: [0.003873755456879735]\n",
      "\tVal Loss is: [8.192610897123814]\n",
      "\tSpeed is: [487.7449304484742] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:55:53]--Step/Epoch/Total: [9400/22/100000]\n",
      "\tTraining Loss is: [0.0008512478088960052]\n",
      "\tVal Loss is: [8.178496958687901]\n",
      "\tSpeed is: [504.6641351063881] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:57:16]--Step/Epoch/Total: [9600/23/100000]\n",
      "\tTraining Loss is: [0.004169338382780552]\n",
      "\tVal Loss is: [8.21172272413969]\n",
      "\tSpeed is: [499.16443026902226] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-07:58:38]--Step/Epoch/Total: [9800/23/100000]\n",
      "\tTraining Loss is: [0.008715188130736351]\n",
      "\tVal Loss is: [8.232834283262491]\n",
      "\tSpeed is: [420.56044591818215] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-08:00:00]--Step/Epoch/Total: [10000/24/100000]\n",
      "\tTraining Loss is: [0.002707916544750333]\n",
      "\tVal Loss is: [8.207618545740843]\n",
      "\tSpeed is: [512.215715847128] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-08:01:55]--Step/Epoch/Total: [10200/24/100000]\n",
      "\tTraining Loss is: [0.0009274050826206803]\n",
      "\tVal Loss is: [8.19031410291791]\n",
      "\tSpeed is: [512.2038311144328] Samples/Secs\n",
      "\tWriting to the file: log.txt\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Learning rate is:  0.001\n",
      "Time: [2018/10/23-08:03:18]--Step/Epoch/Total: [10400/25/100000]\n",
      "\tTraining Loss is: [0.001611448242329061]\n",
      "\tVal Loss is: [8.21019322797656]\n",
      "\tSpeed is: [507.39676179968836] Samples/Secs\n",
      "\tWriting to the file: log.txt\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "\n",
    "print('Start training!!')\n",
    "for i in range(EPOCH_NUMS):\n",
    "    for X, Y in train_loader:\n",
    "        start = time.time()\n",
    "        X = X.numpy()\n",
    "        X = X.reshape((-1, imgH, imgW, 1))\n",
    "#         print('Y ',Y)\n",
    "        Y = np.array(Y)\n",
    "        Length = int(imgW / 4) - 2\n",
    "        batch = X.shape[0]\n",
    "        X_train, Y_train = [\n",
    "            X, Y, np.ones(batch) * Length,\n",
    "            np.ones(batch) * n_len\n",
    "        ], np.ones(batch)\n",
    "#         print(Y_train)\n",
    "        model.train_on_batch(X_train, Y_train)\n",
    "        j +=1 \n",
    "        if j % interval == 0:\n",
    "            times = time.time() - start\n",
    "            currentLoss_train = model.evaluate(X_train, Y_train)\n",
    "            crrentLoss = 0\n",
    "            for X,Y in test_loader:\n",
    "                X = X.numpy()\n",
    "                X = X.reshape((-1, imgH, imgW, 1))\n",
    "                Y = Y.numpy()\n",
    "                Y = np.array(Y)\n",
    "                batch = X.shape[0]\n",
    "                X_val, Y_val = [\n",
    "                    X, Y, np.ones(batch) * Length,\n",
    "                    np.ones(batch) * n_len\n",
    "                ], np.ones(batch)\n",
    "                crrentLoss += model.test_on_batch(X_val, Y_val)\n",
    "            print('Learning rate is: ', LEARNING_RATE)\n",
    "            now_time = time.strftime('%Y/%m/%d-%H:%M:%S',\n",
    "                                     time.localtime(time.time()))\n",
    "            print('Time: [%s]--Step/Epoch/Total: [%d/%d/%d]' % (now_time, j, i,\n",
    "                                                                EPOCH_NUMS))\n",
    "            print('\\tTraining Loss is: [{}]'.format(currentLoss_train))\n",
    "            print('\\tVal Loss is: [{}]'.format(crrentLoss))\n",
    "            print('\\tSpeed is: [{}] Samples/Secs'.format(interval / times))\n",
    "            path = MODEL_PATH + '/num_char_%06d_10.h5'%(crrentLoss*10)\n",
    "            with open(LOG_FILE, mode='a') as log_file:\n",
    "                log_str = now_time + '----global_step:' + str(\n",
    "                    j) + '----loss:' + str(loss) + '\\n'\n",
    "                log_file.writelines(log_str)\n",
    "            log_file.close()\n",
    "            print('\\tWriting to the file: log.txt')\n",
    "            if crrentLoss < loss:\n",
    "                loss = crrentLoss\n",
    "                print(\"\\tSave model to disk: {}\".format(path))\n",
    "                model.save(path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
